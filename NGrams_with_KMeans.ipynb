{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d63dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some notes\n",
    "#1. Doesn't look like a player receive a curse is treated as an action \n",
    "#2. Should I add round end markers?\n",
    "#3. Does normalising the ngram probabilities with the number of events in the trace make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d792f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anthonyowen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "from nltk import ngrams\n",
    "from nltk.probability import FreqDist\n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "#define number of NGrams to use\n",
    "N = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc952cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       GameID  Player  Round  Turn              ActionDescription\n",
      "0           2       0      0     0              End Current Phase\n",
      "1           2       0      0     0    BuyCard: SILVER by player 0\n",
      "2           2       1      0     1              End Current Phase\n",
      "3           2       1      0     1  BuyCard: WORKSHOP by player 1\n",
      "4           2       0      1     0              End Current Phase\n",
      "...       ...     ...    ...   ...                            ...\n",
      "27274     201       0     20     0    BuyCard: ESTATE by player 0\n",
      "27275     201       1     20     1              End Current Phase\n",
      "27276     201       1     20     1    BuyCard: SENTRY by player 1\n",
      "27277     201       0     21     0              End Current Phase\n",
      "27278     201       0     21     0    BuyCard: ESTATE by player 0\n",
      "\n",
      "[27279 rows x 5 columns]\n",
      "Index(['GameID', 'Player', 'Round', 'Turn', 'ActionDescription'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#data  = pd.read_csv(\"data/ActionsReduced_BMWG_vs_DW_GPM100.csv\")\n",
    "data  = pd.read_csv(\"data/ActionsReduced_Budget500_vs_Budget500_GPM100_SD.csv\")\n",
    "data = data[['GameID', 'Player', 'Round','Turn','ActionDescription']]\n",
    "#data  = pd.read_csv(\"data/2Player_FG1E_Supply_AllYears_ActionTraces.csv\")\n",
    "print(data)\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56736119",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGS_FROM_TAG = True\n",
    "NoOfGames = len(data['GameID'].unique())\n",
    "\n",
    "#label all games with corresponding agent names\n",
    "if LOGS_FROM_TAG == True:\n",
    "    agents = ['MediumA', 'MediumB']\n",
    "    games_per_matchup = 100\n",
    "    self_play = False\n",
    "    \n",
    "    #first generate match-ups\n",
    "    matchups = []\n",
    "    if self_play:\n",
    "        for agent1 in agents:\n",
    "            for agent2 in agents:\n",
    "                matchups.append((agent1, agent2))\n",
    "    else:\n",
    "        matchups = list(permutations(agents, 2))\n",
    "\n",
    "    #function to map gameID to match-up\n",
    "    def gameID_to_matchup(game_id, player_no, matchup_list, no_games_per_matchup, min_game_id):\n",
    "        game_group = int((game_id - min_game_id)/no_games_per_matchup)\n",
    "        matchup = matchup_list[game_group]\n",
    "        agent1, agent2 = matchup\n",
    "        if player_no == 0:\n",
    "            return agent1\n",
    "        else:\n",
    "            return agent2\n",
    "\n",
    "    #add agent names to data set\n",
    "    min_GameID = data['GameID'].min()\n",
    "    data['AgentName'] = data.apply(lambda row: gameID_to_matchup(row['GameID'], row['Player'], matchups, games_per_matchup, min_GameID), axis = 1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83be4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kingdom card types\n",
    "card_types_SD = ['ARTISAN', 'BANDIT', 'BUREAUCRAT', 'CHAPEL', 'FESTIVAL', 'GARDENS', 'SENTRY', 'THRONE_ROOM', 'WITCH',\n",
    "                 'WORKSHOP', 'CURSE', 'PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
    "card_types_SD_no_curse = ['ARTISAN', 'BANDIT', 'BUREAUCRAT', 'CHAPEL', 'FESTIVAL', 'GARDENS', 'SENTRY', 'THRONE_ROOM', 'WITCH',\n",
    "                 'WORKSHOP','PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
    "card_types_FG1E = ['CELLAR','MARKET','MILITIA','MINE','MOAT','REMODEL','SMITHY','VILLAGE',\n",
    "                'WOODCUTTER','WORKSHOP','CURSE','PROVINCE', 'DUCHY', 'ESTATE', 'GOLD', 'SILVER', 'COPPER']\n",
    "#card_types = card_types_FG1E\n",
    "card_types = card_types_SD_no_curse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5772e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first format action decription\n",
    "def format_action(action, cardtypes):\n",
    "    #there are ... types of actions we need to format\n",
    "    # 1. 'End Current Phase' goes to ECP\n",
    "    # 2. 'BuyCard: CARDX by player Y' goes to BUYCARDX_Y\n",
    "    # 3. 'CARDX : Player Y' goes to PLAYCARDX_Y\n",
    "    \n",
    "    #to identify these we use regular expressions\n",
    "    pattern1 = re.compile(r'End Current Phase')\n",
    "    pattern2 = re.compile(r'BuyCard: (' + '|'.join(cardtypes) + r') by player (0|1)')\n",
    "    pattern3 = re.compile(r'(' + '|'.join(cardtypes) + r') : Player (0|1)')\n",
    "   \n",
    "    match1 = pattern1.match(action)\n",
    "    match2 = None\n",
    "    match3 = None\n",
    "    if match1 == None:\n",
    "        match2 = pattern2.match(action)\n",
    "    if match2 == None:\n",
    "        match3 =  pattern3.match(action)\n",
    "        \n",
    "    if match1 != None:\n",
    "        formatted_action = 'ECP'\n",
    "    elif match2 != None:\n",
    "        matched_card = match2.group(1)\n",
    "        player = match2.group(2)\n",
    "        formatted_action = 'BUY' + matched_card #+ '_' + str(player)\n",
    "    elif match3 != None:\n",
    "        matched_card = match3.group(1)\n",
    "        player = match3.group(2)\n",
    "        formatted_action = 'PLAY' + matched_card #+ '_' + str(player)\n",
    "    else:\n",
    "        pdb.set_trace()\n",
    "        raise Exception(\"Can't match action description\")\n",
    "        \n",
    "    return formatted_action\n",
    "\n",
    "#print(format_action('GOLD : Player 1', card_types_SD))\n",
    "#print(format_action('BuyCard: PROVINCE by player 1', card_types))\n",
    "#print(format_action('End Current Phase', card_types_SD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a3f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/var/folders/60/sk8yhxb56ld6twrcnr2njl3h0000gn/T/ipykernel_8481/3187238519.py\u001b[0m(33)\u001b[0;36mformat_action\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     31 \u001b[0;31m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     32 \u001b[0;31m        \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 33 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't match action description\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     34 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     35 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mformatted_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> action\n",
      "'GainCard: SILVER by player 1'\n"
     ]
    }
   ],
   "source": [
    "#process input file\n",
    "data['ProcAction'] = data.apply(lambda row: format_action(row['ActionDescription'], card_types), axis = 1)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac4842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of all possible actions - note we are ignoring responses to opponents cards (for SD this means just \n",
    "#ignoring receiving curse cards\n",
    "\n",
    "buy_actions = ['BUY' + str(card) for card in card_types]\n",
    "play_actions = ['PLAY' + str(card) for card in card_types]\n",
    "all_actions_list = ['ECP'] + buy_actions + play_actions\n",
    "print(\"Action list length: \" + str(len(all_actions_list)))\n",
    "#print(all_actions_list)\n",
    "\n",
    "#create list of all possible N-grams\n",
    "all_ngrams_list = list(product(all_actions_list, repeat=N))\n",
    "#print(all_ngrams_list)\n",
    "print(\"N-gram list length: \" + str(len(all_ngrams_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b11ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert input data so that each row contains gameID, player and then a list of ngrams \n",
    "#corresponding to the trace\n",
    "traces = data.groupby(['GameID', 'Player','AgentName'])['ProcAction'].agg(lambda x: ' '.join(x)).reset_index()\n",
    "traces['NGrams'] = traces.apply(lambda row: list(ngrams(nltk.word_tokenize(row['ProcAction']),N)), axis = 1)\n",
    "print(traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b646b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute N-gram probabilities, returns either an array with probability values\n",
    "#in the same order as ngrams in ngrams_all, or a dictionary with the n-grams as key\n",
    "#Unobserved ngrams (i.e. ngrams in ngrams_all, that are not in the trace) are assigned\n",
    "#a default probability of zero.\n",
    "def calc_probabilities(ngrams_trace, ngrams_all, convertToArray = False):\n",
    "    # Compute the frequency of ngrams in the trace\n",
    "    frequency_counter = Counter(ngrams_trace)\n",
    "    \n",
    "    #calculate frequencies of all ngrams in ngrams_all that appear in the playtrace\n",
    "    event_count = {gram: frequency_counter.get(gram, 0) for gram in ngrams_all}\n",
    "    \n",
    "    #normalise each entry with the number of n-grams observed for that trace, to convert\n",
    "    #counts into probabilities\n",
    "    probs = {key: value / (1.0*len(ngrams_trace)) for key, value in event_count.items()}\n",
    "    \n",
    "    if convertToArray:\n",
    "        probs = np.array(list(probs.values()))\n",
    "        \n",
    "    return probs\n",
    "\n",
    "#function to take a probability dictionary and create an array\n",
    "def prob_dict_to_array(prob_dict):\n",
    "    return np.array(list(prob_dict.values()))\n",
    "    \n",
    "#funciton to take probability array and convert to dictionary with n-grams as keys\n",
    "#assumes ordering has been maintained\n",
    "def prob_array_to_dict(prob_array, ngrams_all):\n",
    "    prob_dict = {}\n",
    "    index = 0\n",
    "    for gram in ngrams_all:\n",
    "        prob_dict[gram] = prob_array[index]\n",
    "        index+=1\n",
    "    return prob_dict\n",
    "        \n",
    "#find the common set of ngrams between two probability dictionaries, with probabilities\n",
    "#above a given threshold\n",
    "def return_common_ngrams_above_threshold(prob_dict1, prob_dict2, threshold):\n",
    "    common_ngrams = []\n",
    "    #look for entries in the first dictionary with non-zero values\n",
    "    for key, value in prob_dict1.items():\n",
    "        if value > threshold:\n",
    "            common_ngrams.append(key)\n",
    "    #repeat for the second dictionary but avoiding duplicates\n",
    "    for key, value in prob_dict2.items():\n",
    "        if (value > threshold) and (key not in common_ngrams):\n",
    "             common_ngrams.append(key)\n",
    "    return common_ngrams\n",
    "\n",
    "#convert a list of ngram tuples into a list of strings\n",
    "def convert_ngram_tuples_to_strings(ngrams_list):\n",
    "    ngrams_str = []\n",
    "    for tuple_item in ngrams_list:\n",
    "        tuple_str = ''\n",
    "        for index, element in enumerate(tuple_item):\n",
    "            if index != (len(tuple_item)-1):\n",
    "                tuple_str += element + '|'\n",
    "            else:\n",
    "                tuple_str += element\n",
    "        ngrams_str.append(tuple_str)\n",
    "    return ngrams_str\n",
    "\n",
    "#function to calculate Jensen-Shannon distance\n",
    "def kl_divergence(p, q):\n",
    "    eps = 0.00000001\n",
    "    return np.sum(np.where(p < eps, 0, np.where(q < eps,0, p * np.log(p / (1.0*q)))))\n",
    "\n",
    "def jensen_shannon_distance(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * (kl_divergence(p, m) + kl_divergence(q, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5d703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columns to trace data containing arrays for probability data\n",
    "traces['ProbDict'] = traces.apply(lambda row: calc_probabilities(row['NGrams'], all_ngrams_list, False), axis = 1)\n",
    "traces['ProbArray'] = traces.apply(lambda row: calc_probabilities(row['NGrams'], all_ngrams_list, True), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict = traces['ProbDict'].iloc[0]\n",
    "example_array = traces['ProbArray'].iloc[0]\n",
    "#check translation functions work\n",
    "example_dict_converted_to_array = prob_dict_to_array(example_dict)\n",
    "example_array_converted_to_dict = prob_array_to_dict(example_array, all_ngrams_list)\n",
    "    \n",
    "print(np.array_equal(example_dict_converted_to_array, example_array))\n",
    "print(example_array_converted_to_dict == example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate distance matrix for all pairwise trace combinations\n",
    "def symm_distance_matrix(df, distance_func, colname):\n",
    "    traces = df[colname].tolist()\n",
    "    index_combinations = list(combinations(range(len(traces)), 2))\n",
    "\n",
    "    distance_values = [distance_func(traces[i],traces[j]) for i, j in index_combinations]\n",
    "    \n",
    "    num_rows = len(df)\n",
    "    distance_matrix = pd.DataFrame(index=range(num_rows), columns=range(num_rows))\n",
    "    \n",
    "    for (i, j), distance_value in zip(index_combinations, distance_values):\n",
    "        distance_matrix.at[i, j] = distance_value\n",
    "        distance_matrix.at[j, i] = distance_value  # mirror the value\n",
    "    \n",
    "    return distance_matrix.fillna(0)  # fill NaN values with zeros for diagonal elements\n",
    "\n",
    "js_dist_matrix = symm_distance_matrix(traces, jensen_shannon_distance, 'ProbArray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we have the distance matrix we can perform K-means clustering. We start by looking at\n",
    "#inertia and elbow method\n",
    "#inertia and elbow method\n",
    "range_n_clusters = range(1, 5, 1)\n",
    "inertia_vals = []\n",
    "for num_clusters in range_n_clusters:\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    clusterer.fit(js_dist_matrix)\n",
    "    \n",
    "    #clusterer = KMeans(n_clusters=n_clusters, init='k-means++', n_init= 'warn', max_iter=300, tol=0.0001,\n",
    "    #                verbose=0, random_state= 10, copy_x=True, algorithm='lloyd')\n",
    "    \n",
    "    #cluster_labels = clusterer.fit_predict(trace_X)\n",
    "    inertia_vals.append(clusterer.inertia_)\n",
    "\n",
    "#scale the inertia vals so that the first value is one\n",
    "inertia_vals = np.array(inertia_vals)/inertia_vals[0]\n",
    "\n",
    "#plot as a line plot\n",
    "plt.plot(range_n_clusters, inertia_vals)\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Scaled inertia\")\n",
    "\n",
    "#output inertia values\n",
    "print(inertia_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefc55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#silhouette analysis\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "silhouette_avg_list = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 1 columns\n",
    "    fig, ax1 = plt.subplots(1,1)\n",
    "    fig.set_size_inches(7, 3.5)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(js_dist_matrix) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    clusterer.fit(js_dist_matrix)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(js_dist_matrix, clusterer.labels_, metric = 'precomputed')\n",
    "    silhouette_avg_list.append(silhouette_avg)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "    \n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(js_dist_matrix, clusterer.labels_, metric = 'precomputed')\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[clusterer.labels_ == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    #ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "    plt.suptitle(\n",
    "        #\"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n",
    "        \"Number of clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#also we plot just the average silhouette values by cluster number\n",
    "plt.plot(range_n_clusters, silhouette_avg_list)\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Silhouette Average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3dd683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#examine centroids for two clusters\n",
    "num_clusters = 2\n",
    "clusterer = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "# Fit the model to the data\n",
    "clusterer.fit(js_dist_matrix)\n",
    "\n",
    "#add cluster label to traces\n",
    "traces['ClusterLabel'] = clusterer.labels_\n",
    "\n",
    "#split data by cluster label\n",
    "X_0 =traces[traces['ClusterLabel'] == 0]['ProbArray']\n",
    "X_1 =traces[traces['ClusterLabel'] == 1]['ProbArray']\n",
    "\n",
    "#next we can compute the average probability array for each cluster\n",
    "average_prob_arr_cluster0 = np.mean(X_0, axis = 0)\n",
    "average_prob_arr_cluster1 = np.mean(X_1, axis = 0)\n",
    "\n",
    "#and convert them into dictionaries\n",
    "average_prob_dict_cluster0 = prob_array_to_dict(average_prob_arr_cluster0, all_ngrams_list)\n",
    "average_prob_dict_cluster1 = prob_array_to_dict(average_prob_arr_cluster1, all_ngrams_list)\n",
    "\n",
    "#finally check they are still normalised\n",
    "print(np.sum(average_prob_arr_cluster0))\n",
    "print(np.sum(average_prob_arr_cluster1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which agents we have in which cluster\n",
    "Agents_0 = traces[traces['ClusterLabel'] == 0]['AgentName']\n",
    "Agents_1 = traces[traces['ClusterLabel'] == 1]['AgentName']\n",
    "\n",
    "# Use Counter to count frequencies\n",
    "frequency_count_0 = Counter(Agents_0)\n",
    "frequency_count_1 = Counter(Agents_1)\n",
    "\n",
    "# Print the frequency count\n",
    "for value, count in frequency_count_0.items():\n",
    "    print(f\"{value}: {count} times\")\n",
    "    \n",
    "for value, count in frequency_count_1.items():\n",
    "    print(f\"{value}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b125e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot two distributions side by side\n",
    "def plot_distriution_comparison(distribution_dict_1, distribution_dict_2, label1 = '', label2 = '', threshold = 0):\n",
    "    #find a common domain where all probbaility values are greater than a given threshold\n",
    "    common_ngrams = return_common_ngrams_above_threshold(distribution_dict_1, distribution_dict_2, threshold)\n",
    "\n",
    "    #extract probability arrays for these common n-grams\n",
    "    distribution_dict_1_reduced = {key: distribution_dict_1[key] for key in common_ngrams}\n",
    "    distribution_dict_2_reduced = {key: distribution_dict_2[key] for key in common_ngrams}\n",
    "    distribution_array_1_reduced = prob_dict_to_array(distribution_dict_1_reduced)\n",
    "    distribution_array_2_reduced = prob_dict_to_array(distribution_dict_2_reduced)\n",
    "\n",
    "    #next plot probability distributions\n",
    "\n",
    "    #need to convert common_ngrams into a list of strings as opposed to tuples containing strings\n",
    "    common_ngrams_str = convert_ngram_tuples_to_strings(common_ngrams)\n",
    "\n",
    "    #plot discrete probability distributions side by side\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Calculate the x-coordinates for the bars\n",
    "    x_values1 = np.arange(len(common_ngrams_str))\n",
    "    x_values2 = x_values1 + bar_width\n",
    "\n",
    "    plt.bar(x_values1, distribution_array_1_reduced, width=bar_width, label = label1) \n",
    "    plt.bar(x_values2, distribution_array_2_reduced, width=bar_width, label = label2)\n",
    "\n",
    "    plt.xticks(x_values1 + bar_width / 2, common_ngrams_str)\n",
    "    plt.xticks(rotation=90) \n",
    "    plt.ylim(threshold)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_distriution_comparison(average_prob_dict_cluster0, average_prob_dict_cluster1, 'Cluster 0', 'Cluster 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa04f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot example probability distribution for a given play trace for illustration purposes\n",
    "prob_dict = traces['ProbDict'].iloc[0] \n",
    "prob_dict_nonzero = {gram: value for gram, value in prob_dict.items() if value > 0}\n",
    "xvals = convert_ngram_tuples_to_strings(prob_dict_nonzero.keys())\n",
    "yvals = prob_dict_nonzero.values()\n",
    "plt.bar(xvals, yvals, label = 'DW strategy')\n",
    "plt.xticks(rotation=90) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fdf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#also compute centroid using weighted average as opposed to direct average\n",
    "def calculate_centroid(vectors, dist_func, max_iterations=100, tolerance=1e-6):\n",
    "    #Iteratively update centroid\n",
    "    centroid = np.mean(vectors, axis=0)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Step 2: Calculate distance to centorid\n",
    "        distances = np.array([dist_func(centroid, vec) for vec in vectors])\n",
    "\n",
    "        # Step 3: Weighted sum based on inverse distances\n",
    "        weights = 1 / (distances + 1e-9)  # Small constant added to avoid division by zero\n",
    "        #weighted_sum = np.sum(weights[:, np.newaxis] * vectors, axis=0)\n",
    "        weighted_sum = np.sum(weights * vectors, axis=0)\n",
    "\n",
    "        # Step 4: Normalize the result\n",
    "        new_centroid = weighted_sum / np.sum(weights)\n",
    "\n",
    "        # Check convergence\n",
    "        if np.linalg.norm(new_centroid - centroid) < tolerance:\n",
    "            break\n",
    "\n",
    "        centroid = new_centroid\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recompute cluster 0 centroid using weighted averages and compare to direct average\n",
    "X_0 =traces[traces['ClusterLabel'] == 0]['ProbArray']\n",
    "\n",
    "weighted_avg_centroid = calculate_centroid(X_0, jensen_shannon_distance)\n",
    "straight_avg_centroid = np.mean(X_0, axis = 0)\n",
    "\n",
    "#plot side by side\n",
    "weighted_avg_centroid_dict = prob_array_to_dict(weighted_avg_centroid, all_ngrams_list)\n",
    "straight_avg_centroid_dict = prob_array_to_dict(straight_avg_centroid, all_ngrams_list)\n",
    "plot_distriution_comparison(weighted_avg_centroid_dict, straight_avg_centroid_dict, 'Weighted average', 'Average')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
